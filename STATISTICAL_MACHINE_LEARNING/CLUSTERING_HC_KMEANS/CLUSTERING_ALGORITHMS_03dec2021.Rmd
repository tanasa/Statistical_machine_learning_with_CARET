---
title: "CLUSTERING ALGORITHMS on STUDENT DATA"
author: "Bogdan Tanasa"
date: ""
output:
  pdf_document: default
  html_document: default
always_allow_html: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#### **1. INTRODUCTION**

<p>&nbsp;</p>

#### **2. DATA EXPLORATION**

<p>&nbsp;</p>

#### **3. DATA SELECTION**

<p>&nbsp;</p>

#### **4. DATA FILTERING**

<p>&nbsp;</p>

#### **5. K-MEANS CLUSTERING**

<p>&nbsp;</p>

#### **6. HIERARCHICAL CLUSTERING**

<p>&nbsp;</p>

#### **7. HIERARCHICAL CLUSTERING (AGNES and DIANA)**

<p>&nbsp;</p>

#### **8. HIERARCHICAL CLUSTERING (THE HEATMAPS)**

<p>&nbsp;</p>

#### **9. HIERARCHICAL CLUSTERING (CLUSTER TENDENCY)**

<p>&nbsp;</p>

#### **10. CONCLUSIONS**

<p>&nbsp;</p>



<p>&nbsp;</p>

\newpage
<p>&nbsp;</p>
#### **1. INTRODUCTION**
<p>&nbsp;</p>

&nbsp;

&nbsp;

We are using the data from **UCI** : !( https://archive.ics.uci.edu/ml/datasets/Student+Performance )

&nbsp;

&nbsp;

We are reading a file about **STUDENTS**, and we aim to predict whether they have passed or not 
the exams **(PASS/no_PASS)**;

The attributes in the **INPUT FILE** are the following: 

* 1 school - student's school (binary: "GP" - Gabriel Pereira or "MS" - Mousinho da Silveira)

* 2 sex - student's sex (binary: "F" - female or "M" - male)

* 3 age - student's age (numeric: from 15 to 22)

* 4 address - student's home address type (binary: "U" - urban or "R" - rural)

* 5 famsize - family size (binary: "LE3" - less or equal to 3 or "GT3" - greater than 3)

* 6 Pstatus - parent's cohabitation status (binary: "T" - living together or "A" - apart)

* 7 Medu - mother's education (numeric: 0 - none,  1 - primary education (4th grade), 2 - 5th to 9th grade, 3 - secondary education or 4  - higher education)

* 8 Fedu - father's education (numeric: 0 - none,  1 - primary education (4th grade), 2 - 5th to 9th grade, 3 - secondary education or 4 - higher education)

* 9 Mjob - mother's job (nominal: "teacher", "health" care related, civil "services" (e.g. administrative or police), "at_home" or "other")

* 10 Fjob - father's job (nominal: "teacher", "health" care related, civil "services" (e.g. administrative or police), "at_home" or "other")

* 11 reason - reason to choose this school (nominal: close to "home", school "reputation", "course" preference or "other")

* 12 guardian - student's guardian (nominal: "mother", "father" or "other")

* 13 traveltime - home to school travel time (numeric: 1 - <15 min., 2 - 15 to 30 min., 3 - 30 min. to 1 hour, or 4 - >1 hour)

* 14 studytime - weekly study time (numeric: 1 - <2 hours, 2 - 2 to 5 hours, 3 - 5 to 10 hours, or 4 - >10 hours)

* 15 failures - number of past class failures (numeric: n if 1<=n<3, else 4)

* 16 schoolsup - extra educational support (binary: yes or no)

* 17 famsup - family educational support (binary: yes or no)

* 18 paid - extra paid classes within the course subject (Math or Portuguese) (binary: yes or no)

* 19 activities - extra-curricular activities (binary: yes or no)

* 20 nursery - attended nursery school (binary: yes or no)

* 21 higher - wants to take higher education (binary: yes or no)

* 22 internet - Internet access at home (binary: yes or no)

* 23 romantic - with a romantic relationship (binary: yes or no)

* 24 famrel - quality of family relationships (numeric: from 1 - very bad to 5 - excellent)

* 25 freetime - free time after school (numeric: from 1 - very low to 5 - very high)

* 26 goout - going out with friends (numeric: from 1 - very low to 5 - very high)

* 27 Dalc - workday alcohol consumption (numeric: from 1 - very low to 5 - very high)

* 28 Walc - weekend alcohol consumption (numeric: from 1 - very low to 5 - very high)

* 29 health - current health status (numeric: from 1 - very bad to 5 - very good)

* 30 absences - number of school absences (numeric: from 0 to 93)

&nbsp;

&nbsp;

**NOTES** 

&nbsp;

&nbsp;

**DATA EXPLORATION** and **DATA SELECTION** and **DATA FILTERING** have been presented also in 
the previous documents, and here, we have not fully included all the figures in those sections. 

\newpage
<p>&nbsp;</p>
#### **2. DATA EXPLORATION**
<p>&nbsp;</p>
\newpage

```{r }
options(warn=-1)
suppressPackageStartupMessages(library(ggplot2))
suppressPackageStartupMessages(library(reshape2))
suppressPackageStartupMessages(library(readxl))
suppressPackageStartupMessages(library(dplyr))
suppressPackageStartupMessages(library(tidyr))
suppressPackageStartupMessages(library(purrr))
suppressPackageStartupMessages(library(ggpubr))
suppressPackageStartupMessages(library(broom))
suppressPackageStartupMessages(library(tibble))
suppressPackageStartupMessages(library(class))
suppressPackageStartupMessages(library(gmodels))
suppressPackageStartupMessages(library(caret))
suppressPackageStartupMessages(library(e1071))
suppressPackageStartupMessages(library(ISLR))
suppressPackageStartupMessages(library(pROC))
suppressPackageStartupMessages(library(lattice))
suppressPackageStartupMessages(library(kknn))
suppressPackageStartupMessages(library(multiROC))
suppressPackageStartupMessages(library(MLeval))
suppressPackageStartupMessages(library(AppliedPredictiveModeling))
suppressPackageStartupMessages(library(corrplot))
suppressPackageStartupMessages(library(Hmisc))
suppressPackageStartupMessages(library(rattle))
suppressPackageStartupMessages(library(Hmisc))
suppressPackageStartupMessages(library(broom)) # to add : AUGMENT
suppressPackageStartupMessages(library(rattle))
suppressPackageStartupMessages(library(quantmod)) 
suppressPackageStartupMessages(library(nnet))
suppressPackageStartupMessages(library(NeuralNetTools))
suppressPackageStartupMessages(library(neuralnet))
suppressPackageStartupMessages(library(klaR))
suppressPackageStartupMessages(library(kernlab))
suppressPackageStartupMessages(library(gridExtra))
suppressPackageStartupMessages(library(cluster))
suppressPackageStartupMessages(library(factoextra))
suppressPackageStartupMessages(library(magrittr))
suppressPackageStartupMessages(library(fpc))
suppressPackageStartupMessages(library(gplots))
suppressPackageStartupMessages(library(pheatmap))
# suppressPackageStartupMessages(library(d3heatmap))
suppressPackageStartupMessages(library(clValid))
suppressPackageStartupMessages(library(clustertend))
suppressPackageStartupMessages(library(factoextra))

######################################################
######################################################

FILE1="student.mat.txt"

######################################################
######################################################
# FILE2="student.por.txt"
# FILE3="student.mat.and.por.txt"
######################################################
######################################################

# using the data for CLUSTERING 

######################################################
######################################################

student <- read.delim(FILE1, sep="\t", header=T, stringsAsFactors=F)

######################################################
######################################################

summary(student)
str(student)
class(student)
```

Here we are starting to display the data for visual exploration.

```{r }
################################################################################################
################################################################################################
# 1 school - student's school (binary: "GP" - Gabriel Pereira or "MS" - Mousinho da Silveira)

# unique(student$school)

# ggplot(data = student) + 
#        geom_bar(mapping = aes(x=school, fill=school))

# ggsave("display.1.school.png")
# student$school = as.character(student$school)
student$school = as.factor(student$school)

################################################################################################
################################################################################################
# 2 sex - student's sex (binary: "F" - female or "M" - male)

# unique(student$sex)

# ggplot(data = student) + 
#        geom_bar(mapping = aes(x=sex , fill=sex))

# ggsave("display.2.sex.png")
student$sex = as.factor(student$sex)

################################################################################################
################################################################################################
# 3 age - student's age (numeric: from 15 to 22)

# unique(student$age)

# ggplot(data = student) + 
#        geom_bar(mapping = aes(x=age , fill=age))

# ggplot(data=student, aes(x=age)) + 
#       geom_histogram(aes(y=..density..), colour="black", fill="white")+
#       geom_density(alpha=.2, fill="#FF6666")

# ggsave("display.3.age.png")
# AGE is already on the numerical scale !!  
student$age = as.integer(student$age)

################################################################################################
################################################################################################
# 4 address - student's home address type (binary: "U" - urban or "R" - rural)

# unique(student$address) ## [1] "U" "R"

# ggplot(data = student) + 
#        geom_bar(mapping = aes(x=address, fill=address))

# ggsave("display.4.address.png")
student$address = as.factor(student$address)

################################################################################################
################################################################################################
# 5 famsize - family size (binary: "LE3" - less or equal to 3 or "GT3" - greater than 3)

# unique(student$famsize)

# ggplot(data = student) + 
#        geom_bar(mapping = aes(x=famsize, fill=famsize))

# ggsave("display.5.famsize.png")
student$famsize = as.factor(student$famsize)

################################################################################################
################################################################################################
# 6 Pstatus - parent's cohabitation status (binary: "T" - living together or "A" - apart)

# unique(student$Pstatus)

# ggplot(data = student) + 
#        geom_bar(mapping = aes(x=Pstatus, fill=Pstatus))

# ggsave("display.6.Pstatus.png")
student$Pstatus = as.factor(student$Pstatus)

################################################################################################
################################################################################################
# 7 Medu - mother's education (numeric: 0 - none,  1 - primary education (4th grade), 
# 2 - 5th to 9th grade, 3 - secondary education or 4 - higher education)

# unique(student$Medu)

# ggplot(data = student) + 
#        geom_bar(mapping = aes(x=Medu, fill=Medu))

# ggsave("display.7.Medu.png")
# we may wanna use the numerical values in various regression models
# student$Medu = as.integer(student$Medu)
student$Medu = as.factor(student$Medu)

################################################################################################
################################################################################################
# 8 Fedu - father's education (numeric: 0 - none,  1 - primary education (4th grade), 
# 2 - 5th to 9th grade, 3 - secondary education or 4 - higher education)

# unique(student$Fedu)

# ggplot(data = student) + 
#        geom_bar(mapping = aes(x=Fedu, fill=Fedu))

# ggsave("display.8.Fedu.png")
# we may wanna use the numerical values in various regression models
# student$Fedu = as.integer(student$Fedu)
student$Fedu = as.factor(student$Fedu)

################################################################################################
################################################################################################
# 9 Mjob - mother's job (nominal: "teacher", "health" care related, civil "services" 
# (e.g. administrative or police), "at_home" or "other")

# unique(student$Mjob)

# ggplot(data = student) + 
#        geom_bar(mapping = aes(x=Mjob, fill=Mjob))

# ggsave("display.9.Mjob.png")
student$Mjob = as.factor(student$Mjob)

################################################################################################
################################################################################################
# 10 Fjob - father's job (nominal: "teacher", "health" care related, civil "services" 
# (e.g. administrative or police), "at_home" or "other")

# unique(student$Fjob)

# ggplot(data = student) + 
#        geom_bar(mapping = aes(x=Fjob, fill=Fjob))

# ggsave("display.10.Fjob.png")
student$Fjob = as.factor(student$Fjob)

################################################################################################
################################################################################################
# 11 reason - reason to choose this school 
# (nominal: close to "home", school "reputation", "course" preference or "other")

# unique(student$reason)

# ggplot(data = student) + 
#        geom_bar(mapping = aes(x=reason, fill=reason))

# ggsave("display.11.reason.png")
student$reason = as.factor(student$reason)

################################################################################################
################################################################################################
# 12 guardian - student's guardian (nominal: "mother", "father" or "other")

# unique(student$guardian)

# ggplot(data = student) + 
#        geom_bar(mapping = aes(x=guardian, fill=guardian))

# ggsave("display.12.guardian.png")
student$guardian = as.factor(student$guardian)

################################################################################################
################################################################################################
# 13 traveltime - home to school travel time (numeric: 1 - <15 min., 2 - 15 to 30 min., 
# 3 - 30 min. to 1 hour, or 4 - >1 hour)

# unique(student$traveltime)

# ggplot(data = student) + 
#       geom_bar(mapping = aes(x=traveltime, fill=traveltime))

# ggsave("display.13.traveltime.png")
# student$traveltime = as.factor(student$traveltime)
# we may wanna use the NUMERICAL VALUES :
student$traveltime = as.integer(student$traveltime)

################################################################################################
################################################################################################
# 14 studytime - weekly study time (numeric: 1 - <2 hours, 2 - 2 to 5 hours, 
# 3 - 5 to 10 hours, or 4 - >10 hours)

# unique(student$studytime)

# ggplot(data = student) + 
#        geom_bar(mapping = aes(x=studytime, fill=studytime))

# ggsave("display.14.studytime.png")
# student$studytime = as.factor(student$studytime)
# we may wanna use the NUMERICAL VALUES :
student$studytime = as.integer(student$studytime)

################################################################################################
################################################################################################
# 15 failures - number of past class failures (numeric: n if 1<=n<3, else 4)

# unique(student$failures)

# ggplot(data = student) + 
#       geom_bar(mapping = aes(x=failures, fill=failures))

# ggsave("display.15.failures.png")
# we may wanna use the NUMERICAL VALUES :
student$failures = as.integer(student$failures)

################################################################################################
################################################################################################
# 16 schoolsup - extra educational support (binary: yes or no)

# unique(student$schoolsup)

# ggplot(data = student) + 
#        geom_bar(mapping = aes(x=schoolsup, fill=schoolsup))

# ggsave("display.16.schoolsup.png")
student$schoolsup = as.factor(student$schoolsup)

################################################################################################
################################################################################################
# 17 famsup - family educational support (binary: yes or no)

# unique(student$famsup)

# ggplot(data = student) + 
#        geom_bar(mapping = aes(x=famsup, fill=famsup))

# ggsave("display.17.famsup.png")
student$famsup = as.factor(student$famsup)

################################################################################################
################################################################################################
# 18 paid - extra paid classes within the course subject (Math or Portuguese) 
# (binary: yes or no)

# unique(student$paid)

# ggplot(data = student) + 
#        geom_bar(mapping = aes(x=paid, fill=paid))

# ggsave("display.18.paid.png")
student$paid = as.factor(student$paid)

################################################################################################
################################################################################################
# 19 activities - extra-curricular activities (binary: yes or no)

# unique(student$activities)

# ggplot(data = student) + 
#        geom_bar(mapping = aes(x=activities, fill=activities))

# ggsave("display.19.activities.png")
student$activities = as.factor(student$activities)

################################################################################################
################################################################################################
# 20 nursery - attended nursery school (binary: yes or no)

# unique(student$nursery)

# ggplot(data = student) + 
#        geom_bar(mapping = aes(x=nursery, fill=nursery))

# ggsave("display.20.nursery.png")
student$nursery = as.factor(student$nursery)

################################################################################################
################################################################################################
# 21 higher - wants to take higher education (binary: yes or no)

# unique(student$higher)

# ggplot(data = student) + 
#        geom_bar(mapping = aes(x=higher, fill=higher))

# ggsave("display.21.higher.png")
student$higher = as.factor(student$higher)

################################################################################################
################################################################################################
# 22 internet - Internet access at home (binary: yes or no)

# unique(student$internet)

# ggplot(data = student) + 
#        geom_bar(mapping = aes(x=internet, fill=internet))

# ggsave("display.22.internet.png")
student$internet = as.factor(student$internet)

################################################################################################
################################################################################################
# 23 romantic - with a romantic relationship (binary: yes or no)

# unique(student$romantic)

# ggplot(data = student) + 
#        geom_bar(mapping = aes(x=romantic, fill=romantic))

# ggsave("display.23.romantic.png")
student$romantic = as.factor(student$romantic)

################################################################################################
################################################################################################
# 24 famrel - quality of family relationships (numeric: from 1 - very bad to 5 - excellent)

# unique(student$famrel)

# ggplot(data = student) + 
#        geom_bar(mapping = aes(x=famrel, fill=famrel))

# ggsave("display.24.famrel.png")
# i believe that we can keep these as numerical : or factor ?
student$famrel = as.factor(student$famrel)

################################################################################################
################################################################################################
# 25 freetime - free time after school (numeric: from 1 - very low to 5 - very high)

# unique(student$freetime)

# ggplot(data = student) + 
#       geom_bar(mapping = aes(x=freetime, fill=freetime))

# ggsave("display.25.freetime.png")
# i believe that we can keep these as numerical :
student$freetime = as.factor(student$freetime)

################################################################################################
################################################################################################
# 26 goout - going out with friends (numeric: from 1 - very low to 5 - very high)

# unique(student$goout)

# ggplot(data = student) + 
#        geom_bar(mapping = aes(x=goout, fill=goout))

# ggsave("display.26.goout.png")
# i believe that we can keep these as numerical :
student$goout = as.factor(student$goout)

################################################################################################
################################################################################################
# 27 Dalc - workday alcohol consumption (numeric: from 1 - very low to 5 - very high)

# unique(student$Dalc)

# ggplot(data = student) + 
#        geom_bar(mapping = aes(x=Dalc, fill=Dalc))

# ggsave("display.27.Dalc.png")
# i believe that we can keep these as numerical :
student$Dalc = as.factor(student$Dalc)

################################################################################################
################################################################################################
# 28 Walc - weekend alcohol consumption (numeric: from 1 - very low to 5 - very high)

# unique(student$Walc)

# ggplot(data = student) + 
#        geom_bar(mapping = aes(x=Walc, fill=Walc))

# ggsave("display.28.Walc.png")
# i believe that we can keep these as numerical :
student$Walc = as.factor(student$Walc)

################################################################################################
################################################################################################
# 29 health - current health status (numeric: from 1 - very bad to 5 - very good)

# unique(student$health)

# ggplot(data = student) + 
#        geom_bar(mapping = aes(x=health, fill=health))

# ggsave("display.29.health.png")
# i believe that we can keep these as numerical :
student$health = as.factor(student$health)

################################################################################################
################################################################################################
# 30 absences - number of school absences (numeric: from 0 to 93)

# unique(student$absences)

# ggplot(data = student) + 
#       geom_bar(mapping = aes(x=absences, fill=absences))

# ggplot(data=student, aes(x=absences)) + 
#      geom_histogram(aes(y=..density..), colour="black", fill="white")+
#      geom_density(alpha=.2, fill="#FF6666")

# ggsave("display.30.absences.png")
# i believe that we can keep these as numerical :
student$absences = as.integer(student$absences)

################################################################################################
################################################################################################
# $ G1        : int  5 5 7 15 6 15 12 6 16 14 ...

# unique(student$G1)

# ggplot(data = student) + 
#        geom_bar(mapping = aes(x=G1, fill=G1))

# ggplot(data=student, aes(x=G1)) + 
#       geom_histogram(aes(y=..density..), colour="black", fill="white")+
#       geom_density(alpha=.2, fill="#FF6666")

# ggsave("display.0.G1.png")
# i believe that we can keep these as numerical, although we may not need it :
student$G1 = as.integer(student$G1)

################################################################################################
################################################################################################
# $ G2        : int  6 5 8 14 10 15 12 5 18 15 ...

# unique(student$G2)

# ggplot(data = student) + 
#        geom_bar(mapping = aes(x=G2, fill=G2))

# ggplot(data=student, aes(x=G2)) + 
#       geom_histogram(aes(y=..density..), colour="black", fill="white")+
#       geom_density(alpha=.2, fill="#FF6666")

# ggsave("display.0.G2.png")
# i believe that we can keep these as numerical, although we may not need it :
student$G2 = as.integer(student$G2)

################################################################################################
################################################################################################
# $ G3        : int  6 6 10 15 10 15 11 6 19 15 ...

# unique(student$G3)

# ggplot(data = student) + 
#        geom_bar(mapping = aes(x=G3, fill=G3))

# ggplot(data=student, aes(x=G3)) + 
#       geom_histogram(aes(y=..density..), colour="black", fill="white")+
#       geom_density(alpha=.2, fill="#FF6666")

# ggsave("display.0.G3.png")
# i believe that we can covert it into RANGES of VALUES :
student$G3 = as.integer(student$G3)

################################################################################################
summary(student)
str(student)
class(student)
################################################################################################
# knitr::kable(summary(student, format = "html"))
################################################################################################
```

\newpage
<p>&nbsp;</p>
#### **3. DATA SELECTION**
<p>&nbsp;</p>
\newpage

```{r }
## the OUTPUT VARIABLES is G3
## we may remove G1 and G2
## and some other features

student1 <- subset(student, select = -c(G1, G2))

student2 <- subset(student1, 
                   select = -c(school, sex, address, famsize, Pstatus, 
                   Mjob, Fjob, reason, guardian, schoolsup, famsup, 
                   paid, activities, nursery, 
                   higher, internet, romantic))

### shall we decide to keep ALL the FEATURES (ATTRIBUTES)
student2 = student1

str(student2)

### depending on the algorithm that we may choose to use
student2$G3 = as.factor(student2$G3)
### student2$G3 = as.integer(student2$G3)

table(student2$G3)

### for simplicity, to work with a copy of STUDENT2, let's call it STUDENT3

student3 = subset(student2, 
                  select= c(age, traveltime, studytime, failures, absences, G3))

### shall we decide to keep ALL the FEATURES (ATTRIBUTES)
### student3 = student2

table(student3$G3)
```

\newpage
<p>&nbsp;</p>
#### **4. DATA FILTERING**
<p>&nbsp;</p>
\newpage

```{r } 
## in order to KEEP the RECORDS where the GRADE 3 is > 2 :

dim(student3)

student3$G3 = as.integer(student3$G3)

student4 = student3[student3$G3 > 2, ]

dim(student4) 

ggplot(data = student4) + 
       geom_bar(mapping = aes(x=G3, fill=G3))

ggsave("display.0.G3.after.filtering.grade3.frequency.png")

student3 = student4

## FOR CLUSTERING, we may not use the R code below ;

## TRANSFORMING G3 into RANGES of PASS and NO-PASS :

## student3$G3 = as.integer(student3$G3)

## student3$RESULT[student3$G3 <= 10] = "NO_PASS"
## student3$RESULT[student3$G3 >=10 ] = "PASS"

## student3 <- subset(student3, select = -c(G3))

## student3$RESULT = as.factor(student3$RESULT)

## DISPLAYING THE FEATURES (ATTRIBUTES) in THE CURRENT DATASET :

colnames(student3)

summary(student3)
```

\newpage
<p>&nbsp;</p>
#### **5. DATA SCALING**
<p>&nbsp;</p>
\newpage

```{r } 
student3 <- na.omit(student3)
dim(student3)

## in order to SCALE the DATA

student3_scaled = scale(student3)
summary(student3_scaled)
```

\newpage
<p>&nbsp;</p>
#### **5. K-MEANS CLUSTERING**
<p>&nbsp;</p>
\newpage

\newpage
<p>&nbsp;</p>
#### **5.1. K-MEANS CLUSTERING (WSS)**
<p>&nbsp;</p>
\newpage

```{r } 
## Looking at the OPTIMAL NUMBER of CLUSTERS by WSS method
## i.e. "within cluster sums of squares"

optimalclusters_WSS <- fviz_nbclust(student3_scaled, kmeans, method="wss")
print(optimalclusters_WSS)

## Running the K-MEANS clustering algorithm by using 7 CLUSTERS
kmeans.df <- kmeans(student3_scaled, 7, nstart=25)
print(kmeans.df)

## Visualization : 
optimalclusters_WSS_fviz = fviz_cluster(kmeans.df,
                                        data = student3_scaled,
                                        geom = c("point"),  
                                        ggtheme=theme_classic())

gridExtra::grid.arrange(optimalclusters_WSS_fviz)

## Numerical SUMMARY of the CLUSTERS
clusters_aggregate <- aggregate(student3, by=list(cluster=kmeans.df$cluster), mean)
print(clusters_aggregate)

student3 %>%
  mutate(Cluster = kmeans.df$cluster) %>%
  group_by(Cluster) %>%
  summarise_all("mean")

### Just in case that we will need the information on CLUSTERS in the BIG DATA FRAME

clusterbind_student3 <- cbind(student3, kmeans.df$cluster)
head(clusterbind_student3)
tail(clusterbind_student3)
```

\newpage
<p>&nbsp;</p>
#### **5.2. K-MEANS CLUSTERING (GAP)**
<p>&nbsp;</p>
\newpage

```{r } 
## Looking at the OPTIMAL NUMBER of CLUSTERS by GAP method

optimalclusters_GAP <- fviz_nbclust(student3_scaled, kmeans, method="gap_stat", 
                                    nstart=25, nboot=50) + 
                                    labs(subtitle = "Gap Statistic Method")
print(optimalclusters_GAP)

## Running the K-MEANS clustering algorithm by using 7 CLUSTERS
kmeans.df <- kmeans(student3_scaled, 7, nstart=25)
print(kmeans.df)

## Visualization : 
optimalclusters_GAP_fviz = fviz_cluster(kmeans.df,
                                        data = student3_scaled,
                                        geom = c("point"),
                                        ggtheme=theme_classic())

gridExtra::grid.arrange(optimalclusters_GAP_fviz)

## Numerical SUMMARY of the CLUSTERS
clusters_aggregate <- aggregate(student3, by=list(cluster=kmeans.df$cluster), mean)
print(clusters_aggregate)

student3 %>%
  mutate(Cluster = kmeans.df$cluster) %>%
  group_by(Cluster) %>%
  summarise_all("mean")

### just in case that we will need the information on CLUSTERS in the BIG DATA FRAME

clusterbind_student3 <- cbind(student3, kmeans.df$cluster)
head(clusterbind_student3)
tail(clusterbind_student3)
```

\newpage
<p>&nbsp;</p>
#### **5.3. K-MEANS CLUSTERING (SILHOUETTE)**
<p>&nbsp;</p>
\newpage

```{r } 
## Using K-MEANS
## Looking at the OPTIMAL NUMBER of CLUSTERS by SILHOUETTE METHOD

optimalclusters_SILHOUETTE <- fviz_nbclust(student3_scaled, kmeans, nstart=25, 
                                           method="silhouette", nboot=50) + 
                                           labs(subtitle = "Silhouette Method")
print(optimalclusters_SILHOUETTE)

## Running the K-MEANS clustering algorithm by using 2 CLUSTERS
kmeans.df <- kmeans(student3_scaled, 2, nstart=25)
print(kmeans.df)

## Visualization : 
optimalclusters_SILHOUETTE_fviz = fviz_cluster(kmeans.df,
                                        data = student3_scaled,
                                        geom = c("point"),
                                        ggtheme=theme_classic())

gridExtra::grid.arrange(optimalclusters_SILHOUETTE_fviz)

## Numerical SUMMARY of the CLUSTERS
clusters_aggregate <- aggregate(student3, by=list(cluster=kmeans.df$cluster), mean)
print(clusters_aggregate)

student3 %>%
  mutate(Cluster = kmeans.df$cluster) %>%
  group_by(Cluster) %>%
  summarise_all("mean")

### just in case that we will need the information on CLUSTERS in the BIG DATA FRAME

clusterbind_student3 <- cbind(student3, kmeans.df$cluster)
head(clusterbind_student3)
tail(clusterbind_student3)
```

```{r } 
## Comparing the summaries above to the dataset grouped by GRADE G3
## (age, traveltime, studytime, failures, absences, G3)

## tapply(student3$age, student3$G3, summary)
## tapply(student3$traveltime, student3$G3, summary)
## tapply(student3$studytime, student3$G3, summary)
## tapply(student3$failures, student3$G3, summary)
## tapply(student3$absences, student3$G3, summary)
## tapply(student3$G3, student3$G3, summary)

```

\newpage
<p>&nbsp;</p>
#### **5.4. PAM-based CLUSTERING (after SILHOUETTE method)**
<p>&nbsp;</p>
\newpage

```{r } 
## Using PAM Partitioning Around Medoids
## Looking at the OPTIMAL NUMBER of CLUSTERS by SILHOUETTE

optimalclusters_PAM <- fviz_nbclust(student3_scaled, 
                                           pam,                                      
                                           method="silhouette") + 
                                           labs(subtitle = "PAM and Silhouette Method")
print(optimalclusters_PAM)

#  Using "PAM" function for Partitioning (clustering) of the data into ‘k’ clusters 
# "around medoids"", a more robust version of K-means.
```

```{r } 
pam_clusters <- pam(student3_scaled, 2)

print(pam_clusters)
str(pam_clusters)
print(pam_clusters$medoids)

## Visualization : 
optimalclusters_SILHOUETTE_fviz = fviz_cluster(pam_clusters,
                                               data = student3_scaled,
                                               geom = c("point"),
                                               ggtheme=theme_classic())

gridExtra::grid.arrange(optimalclusters_SILHOUETTE_fviz)

## Numerical SUMMARY of the CLUSTERS
pamclusters_aggregate <- aggregate(student3, by=list(cluster=pam_clusters$cluster), mean)
print(pamclusters_aggregate)

student3 %>%
  mutate(Cluster = pam_clusters$cluster) %>%
  group_by(Cluster) %>%
  summarise_all("mean")

### just in case that we will need the information on CLUSTERS in the BIG DATA FRAME

clusterbind_student3 <- cbind(student3, pam_clusters$cluster)
head(clusterbind_student3)
tail(clusterbind_student3)

```

\newpage
<p>&nbsp;</p>
#### **6. HIERARCHICAL CLUSTERING**
<p>&nbsp;</p>
\newpage

\newpage
<p>&nbsp;</p>
#### **6.1 HIERARCHICAL CLUSTERING (using EUCLIDEAN distance)**
<p>&nbsp;</p>
\newpage

```{r } 
student3_dist = dist(student3_scaled, method="euclidean")
# as.matrix(student3_dist)[1:2,1:2]

agg_tree_ward = hclust(d = student3_dist, method="ward.D2")
#print(agg_tree_ward)

# Visualizing the Dendogram
fviz_dend(agg_tree_ward, cex=.5)

# Cutting the tree to create 2 clusters and visualizing it:
agg_tree_warddend <- fviz_dend(agg_tree_ward, cex=.5, k=2, palette = "jco")
agg_tree_warddend

# To access the partition accuracy of the cluster tree (created by hclust()) there should be a strong
# correlation between the original distance matrix and the object linkage distance defined as cophenetic 
# distances. 
# Calculating Cophenetic Distances

agg_cophenetic <- cophenetic(agg_tree_ward)

# head(agg_cophenetic)
# tail(agg_cophenetic)

# Calculating the correlation between Cophenetic Distances and Original Distances for :
cor(student3_dist, agg_cophenetic)

```

&nbsp;
&nbsp;
&nbsp;

\newpage
<p>&nbsp;</p>
#### **6.2 HIERARCHICAL CLUSTERING (using MINKOWSKI distance)**
<p>&nbsp;</p>
\newpage

```{r } 
student3_dist = dist(student3_scaled, method="minkowski")
# as.matrix(student3_dist)[1:2,1:2]

agg_tree_ward = hclust(d = student3_dist, method="average")
#print(agg_tree_ward)

# Visualizing the Dendogram
fviz_dend(agg_tree_ward, cex=.5)

# Cutting the tree to create 2 clusters and visualizing it:
agg_tree_warddend <- fviz_dend(agg_tree_ward, cex=.5, k=2, palette = "jco")
agg_tree_warddend

# To access the partition accuracy of the cluster tree (created by hclust()) there should be a strong
# correlation between the original distance matrix and the object linkage distance defined as cophenetic 
# distances. 
# Calculating Cophenetic Distances

agg_cophenetic <- cophenetic(agg_tree_ward)

# head(agg_cophenetic)
# tail(agg_cophenetic)

# Calculating the correlation between Cophenetic Distances and Original Distances for :
cor(student3_dist, agg_cophenetic)

```

&nbsp;
&nbsp;
&nbsp;

\newpage
<p>&nbsp;</p>
#### **6.3 HIERARCHICAL CLUSTERING (using CANBERRA distance)**
<p>&nbsp;</p>
\newpage

```{r } 
student3_dist = dist(student3_scaled, method="canberra")
# as.matrix(student3_dist)[1:2,1:2]

agg_tree_ward = hclust(d = student3_dist, method="ward.D2")
#print(agg_tree_ward)

# Visualizing the Dendogram
fviz_dend(agg_tree_ward, cex=.5)

# Cutting the tree to create 2 clusters and visualizing it:
agg_tree_warddend <- fviz_dend(agg_tree_ward, cex=.5, k=2, palette = "jco")
agg_tree_warddend

# To access the partition accuracy of the cluster tree (created by hclust()) there should be a strong
# correlation between the original distance matrix and the object linkage distance defined as cophenetic 
# distances. 
# Calculating Cophenetic Distances

agg_cophenetic <- cophenetic(agg_tree_ward)

# head(agg_cophenetic)
# tail(agg_cophenetic)

# Calculating the correlation between Cophenetic Distances and Original Distances for :
cor(student3_dist, agg_cophenetic)

```

&nbsp;
&nbsp;
&nbsp;

\newpage
<p>&nbsp;</p>
#### **6.4 HIERARCHICAL CLUSTERING (using MANHATTAN distance)**
<p>&nbsp;</p>
\newpage

```{r } 
student3_dist = dist(student3_scaled, method="manhattan")
# as.matrix(student3_dist)[1:2,1:2]

agg_tree_ward = hclust(d = student3_dist, method="average")
#print(agg_tree_ward)

# Visualizing the Dendogram
fviz_dend(agg_tree_ward, cex=.5)

# Cutting the tree to create 2 clusters and visualizing it:
agg_tree_warddend <- fviz_dend(agg_tree_ward, cex=.5, k=2, palette = "jco")
agg_tree_warddend

# To access the partition accuracy of the cluster tree (created by hclust()) there should be a strong
# correlation between the original distance matrix and the object linkage distance defined as cophenetic 
# distances. 
# Calculating Cophenetic Distances

agg_cophenetic <- cophenetic(agg_tree_ward)

# head(agg_cophenetic)
# tail(agg_cophenetic)

# Calculating the correlation between Cophenetic Distances and Original Distances for :
cor(student3_dist, agg_cophenetic)
```

&nbsp;
&nbsp;
&nbsp;

\newpage
<p>&nbsp;</p>
#### **7. HIERARCHICAL CLUSTERING (by AGNES and DIANA)**
<p>&nbsp;</p>
\newpage

\newpage
<p>&nbsp;</p>
#### **7.1 HIERARCHICAL CLUSTERING (by AGNES)**
<p>&nbsp;</p>
\newpage

```{r } 
### AGGLOMERATIVE

agnes_cluster <- agnes(x=student3_scaled, stand=TRUE, metric = "euclidean", method="ward")

str(agnes_cluster)
agnes_cluster$ac

agnes_tree <- pltree(agnes_cluster, cex = 0.6, hang = -1, main = "Dendrogram of Agnes")

print(agnes_tree)

plot(as.hclust(agnes_cluster), cex = 0.6, hang = -1)

fviz_dend(agnes_cluster, cex=.6, k=2)
```

\newpage
<p>&nbsp;</p>
#### **7.2 HIERARCHICAL CLUSTERING (by DIANA)**
<p>&nbsp;</p>
\newpage

```{r } 
### Divisive
diana_cluster <- diana(x=student3_scaled, stand=TRUE, metric = "euclidean")

str(diana_cluster)
diana_cluster$dc

diana_tree <- pltree(diana_cluster, cex = 0.6, hang = -1, main = "Dendrogram of Diana")

print(diana_tree)

plot(as.hclust(diana_cluster), cex = 0.6, hang = -1)

fviz_dend(diana_cluster, cex=.6, k=2)

```

\newpage
<p>&nbsp;</p>
#### **8. HIERARCHICAL CLUSTERING (THE HEATMAPS)**
<p>&nbsp;</p>
\newpage

```{r } 
# using HEATMAP : the high values are in red and low in yellow.

heatmap(student3_scaled)

heatmap.2(student3_scaled, 
          scale="none", 
          col=bluered(100), 
          trace = "none", density.info = "none")

# using PHEATMAP

pheatmap(student3_scaled, cutree_rows = 2)

# using D3HEATMAP

# d3heatmap(scale(student3), k_row=4, k_col=2)

```

\newpage
<p>&nbsp;</p>
#### **9. HIERARCHICAL CLUSTERING (CLUSTER TENDENCY)**
<p>&nbsp;</p>
\newpage

```{r } 
## TRANSFORMING G3 into RANGES of PASS and NO-PASS :

student3$G3 = as.integer(student3$G3)

student3$RESULT[student3$G3 <= 10] = "NO_PASS"
student3$RESULT[student3$G3 >=10 ] = "PASS"

student3 <- subset(student3, select = -c(G3))

student3$RESULT = as.factor(student3$RESULT)

## displaying the PCA analysis :

fviz_pca_ind(prcomp(student3_scaled), 
             title="Heart Attack Risk Data",
             habillage = student3$RESULT, 
             palette = "jco",
             geom = "point", ggtheme=theme_classic(), legend="bottom" )
```

```{r } 
# Calculating Hopkins Statistics to check if the data does exhibit inherent patterns :

hopkins(student3_scaled, n=nrow(student3_scaled) - 1)
```

```{r } 
# Visualizing the Dissimilarity Matrix 
# where RED depicts high similarity and BLUE low similarity

fviz_dist(dist(student3_scaled), show_labels = FALSE) + 
         labs(title = "Student3 Data Set")
```

```{r } 
# using : validation="internal"
cluster_method <- c("hierarchical", "kmeans", "pam", "diana", "agnes")

check <- clValid(student3_scaled, 
                 nClust=2:6, 
                 clMethods=cluster_method, validation="internal")

summary(check)
```

```{r } 
# using : validation="stability"
cluster_method <- c("hierarchical", "kmeans", "pam", "diana", "agnes")

check_stability <- clValid(student3_scaled, 
                           nClust=2:6, 
                           clMethods=cluster_method, validation="stability")

optimalScores(check_stability)
```

\newpage
<p>&nbsp;</p>
#### **10. CONCLUSIONS**
<p>&nbsp;</p>
\newpage

Here above we have compared the algorithms that perform the CLUSTERING, particularly K-MEANS, PAM
(PARTITIONING AROUND MEDOIDS) and HC (HIERARCHICAL CLUSTERING).

We could draw several conclusions from our study:

&nbsp;

1. Referring to the optimal number of clusters to be used for the K-MEANS algorithm,  
WSS and GAP methods suggest to call 7 CLUSTERS by K-MEANS, while SILHOUETTE method 
suggests to use 2 CLUSTERS. 

&nbsp;

2. We have employed both PAM and K-MEANS on 2 clusters, although after visual examination, 
PAM does not seem to have worked too well (by visualizing the data on a dimensionality 
reduction plot), in contrast to K-MEANS that has achieved a better separation.

&nbsp;

3. Referring to the COPHENETIC DISTANCES and the set of CLUSTERING METHODS, we obtain 
the  following values for the COPHENETIC DISTANCES (euclidean : 0.47, minkowski : 0.73,
canberra : 0.62, manhattan : 0.78), suggesting that the MANHATTAN DISTANCE may provide 
more accurate results (followed by MINKOWSKI DISTANCE). 

&nbsp;

As we have read in some text books, "It can be argued that a dendrogram is an appropriate summary of 
some data if the correlation between the original distances and the cophenetic distances is high. 
Otherwise, it should simply be viewed as the description of the output of the clustering algorithm."

&nbsp;

4. The DENDROGRAMS and the CLUSTERING data generated by the measures "WARD.D2/euclidean" and
"WARD.D2/canberra" look similar, while the results of the pipelines "Minkowski/average" and
"Manhattan/average" look very similar too. 

&nbsp;

5. The results of AGNES algorithm look more like "WARD.D2/euclidean" and "WARD.D2/canberra",
while the results of DIANA algorithm look more like "Minkowski/average" and "Manhattan/average".

&nbsp;

6. We have also displayed the HEATMAPS using the functions "heatmap.2", "pheatmap" or "d3.heatmap" functions. 
&nbsp;

7. Referring to Hopkins statistics value of the data, it is 0.1850252, suggesting that the data is 
uniformly distributed (according to the interpretation that we can read in Wikipedia "a value close to 1 
tends to indicate the data is highly clustered, random data will tend to result in values around 0.5, 
and uniformly distributed data will tend to result in values close to 0").

Therefore, the lower the number of the clusters is, the better the modelling approach is. 

&nbsp;

8. At the end, we will have compared all these approaches ("hierarchical", "kmeans", "pam", "diana",
"agnes") and we'll have evaluated the performance by using the function *clValid()*.
(https://cran.r-project.org/web/packages/clValid/vignettes/clValid.pdf)

&nbsp;

9. According to the documentation of clValid on "Internal Validation", we recall that "the connectivity 
should be minimized, while both the Dunn Index and the Silhouette Width should be maximized."

We have obtained optimal scores for the "Hierarchical Clustering" approach, using 2 or 3 clusters.

&nbsp;

10. Shall we consider the "Stability Score" (and the associated measures APN, AD, ADM, and FOM), 
we recall that "these measures should be minimized in each case".

In our case, we have obtained optimal scores on PAM (6 clusters) and AGNES (2 clusters).

&nbsp;
\newpage