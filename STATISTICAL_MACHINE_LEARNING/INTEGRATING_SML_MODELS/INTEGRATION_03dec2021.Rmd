---
title: "ENSEMBLE METHODS and SUPER LEARNERS to predict the GRADE"
author: "Bogdan Tanasa"
date: ""
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#### **1. INTRODUCTION**

<p>&nbsp;</p>

#### **2. DATA EXPLORATION**

<p>&nbsp;</p>

#### **3. DATA SELECTION**

<p>&nbsp;</p>

#### **4. DATA FILTERING**

<p>&nbsp;</p>

#### **5. TRAINING AND TEST SETS**

<p>&nbsp;</p>

#### **6. TRAINING AND PREDICTIONS WITH ANN (CARET)**

<p>&nbsp;</p>

#### **7. TRAINING AND PREDICTIONS WITH ANN**

<p>&nbsp;</p>

#### **8. TRAINING AND PREDICTIONS WITH SVM (CARET)**

<p>&nbsp;</p>

#### **9. TRAINING AND PREDICTIONS WITH SVM**

<p>&nbsp;</p>

#### **10. CONCLUSIONS**

<p>&nbsp;</p>

\newpage
<p>&nbsp;</p>
#### **1. INTRODUCTION**
<p>&nbsp;</p>

We are using the data from **UCI** : !( https://archive.ics.uci.edu/ml/datasets/Student+Performance )

We are reading a file about **STUDENTS**, and we aim to predict whether they have passed or not the exams **(PASS/no_PASS)**;

The attributes in the **INPUT FILE** are the following : 

* 1 school - student's school (binary: "GP" - Gabriel Pereira or "MS" - Mousinho da Silveira)

* 2 sex - student's sex (binary: "F" - female or "M" - male)

* 3 age - student's age (numeric: from 15 to 22)

* 4 address - student's home address type (binary: "U" - urban or "R" - rural)

* 5 famsize - family size (binary: "LE3" - less or equal to 3 or "GT3" - greater than 3)

* 6 Pstatus - parent's cohabitation status (binary: "T" - living together or "A" - apart)

* 7 Medu - mother's education (numeric: 0 - none,  1 - primary education (4th grade), 2 - 5th to 9th grade, 3 - secondary education or 4  - higher education)

* 8 Fedu - father's education (numeric: 0 - none,  1 - primary education (4th grade), 2 - 5th to 9th grade, 3 - secondary education or 4 - higher education)

* 9 Mjob - mother's job (nominal: "teacher", "health" care related, civil "services" (e.g. administrative or police), "at_home" or "other")

* 10 Fjob - father's job (nominal: "teacher", "health" care related, civil "services" (e.g. administrative or police), "at_home" or "other")

* 11 reason - reason to choose this school (nominal: close to "home", school "reputation", "course" preference or "other")

* 12 guardian - student's guardian (nominal: "mother", "father" or "other")

* 13 traveltime - home to school travel time (numeric: 1 - <15 min., 2 - 15 to 30 min., 3 - 30 min. to 1 hour, or 4 - >1 hour)

* 14 studytime - weekly study time (numeric: 1 - <2 hours, 2 - 2 to 5 hours, 3 - 5 to 10 hours, or 4 - >10 hours)

* 15 failures - number of past class failures (numeric: n if 1<=n<3, else 4)

* 16 schoolsup - extra educational support (binary: yes or no)

* 17 famsup - family educational support (binary: yes or no)

* 18 paid - extra paid classes within the course subject (Math or Portuguese) (binary: yes or no)

* 19 activities - extra-curricular activities (binary: yes or no)

* 20 nursery - attended nursery school (binary: yes or no)

* 21 higher - wants to take higher education (binary: yes or no)

* 22 internet - Internet access at home (binary: yes or no)

* 23 romantic - with a romantic relationship (binary: yes or no)

* 24 famrel - quality of family relationships (numeric: from 1 - very bad to 5 - excellent)

* 25 freetime - free time after school (numeric: from 1 - very low to 5 - very high)

* 26 goout - going out with friends (numeric: from 1 - very low to 5 - very high)

* 27 Dalc - workday alcohol consumption (numeric: from 1 - very low to 5 - very high)

* 28 Walc - weekend alcohol consumption (numeric: from 1 - very low to 5 - very high)

* 29 health - current health status (numeric: from 1 - very bad to 5 - very good)

* 30 absences - number of school absences (numeric: from 0 to 93)



**NOTES** 



**DATA EXPLORATION** and **DATA SELECTION** and **DATA FILTERING** have been presented also in
the previous documents, and here, we have not fully included all the figures in those sections. 

\newpage
<p>&nbsp;</p>
#### **2. DATA EXPLORATION**
<p>&nbsp;</p>
\newpage

```{r }
options(warn=-1)
suppressPackageStartupMessages(library(ggplot2))
suppressPackageStartupMessages(library(reshape2))
suppressPackageStartupMessages(library(readxl))
suppressPackageStartupMessages(library(dplyr))
suppressPackageStartupMessages(library(tidyr))
suppressPackageStartupMessages(library(purrr))
suppressPackageStartupMessages(library(ggpubr))
suppressPackageStartupMessages(library(broom))
suppressPackageStartupMessages(library(tibble))
suppressPackageStartupMessages(library(class))
suppressPackageStartupMessages(library(gmodels))
suppressPackageStartupMessages(library(caret))
suppressPackageStartupMessages(library(e1071))
suppressPackageStartupMessages(library(ISLR))
suppressPackageStartupMessages(library(pROC))
suppressPackageStartupMessages(library(lattice))
suppressPackageStartupMessages(library(kknn))
suppressPackageStartupMessages(library(multiROC))
suppressPackageStartupMessages(library(MLeval))
suppressPackageStartupMessages(library(AppliedPredictiveModeling))
suppressPackageStartupMessages(library(corrplot))
suppressPackageStartupMessages(library(Hmisc))
suppressPackageStartupMessages(library(rattle))
suppressPackageStartupMessages(library(Hmisc))
suppressPackageStartupMessages(library(broom)) # to add : AUGMENT
suppressPackageStartupMessages(library(rattle))
suppressPackageStartupMessages(library(quantmod)) 
suppressPackageStartupMessages(library(nnet))
suppressPackageStartupMessages(library(NeuralNetTools))
suppressPackageStartupMessages(library(neuralnet))
suppressPackageStartupMessages(library(klaR))
suppressPackageStartupMessages(library(kernlab))

######################################################
######################################################

FILE1="student.mat.txt"

######################################################
######################################################
# FILE2="student.por.txt"
# FILE3="student.mat.and.por.txt"
######################################################
######################################################

######################################################
######################################################

student <- read.delim(FILE1, sep="\t", header=T, stringsAsFactors=F)

######################################################
######################################################

summary(student)
str(student)
class(student)
```

Here we are starting to display the data for visual exploration.

```{r }
################################################################################################
################################################################################################
# 1 school - student's school (binary: "GP" - Gabriel Pereira or "MS" - Mousinho da Silveira)

# unique(student$school)

# ggplot(data = student) + 
#        geom_bar(mapping = aes(x=school, fill=school))

# ggsave("display.1.school.png")
# student$school = as.factor(student$school)
student$school = as.character(student$school)

################################################################################################
################################################################################################
# 2 sex - student's sex (binary: "F" - female or "M" - male)

# unique(student$sex)

# ggplot(data = student) + 
#        geom_bar(mapping = aes(x=sex , fill=sex))

# ggsave("display.2.sex.png")
student$sex = as.factor(student$sex)

################################################################################################
################################################################################################
# 3 age - student's age (numeric: from 15 to 22)

# unique(student$age)

# ggplot(data = student) + 
#        geom_bar(mapping = aes(x=age , fill=age))

# ggplot(data=student, aes(x=age)) + 
#       geom_histogram(aes(y=..density..), colour="black", fill="white")+
#       geom_density(alpha=.2, fill="#FF6666")

# ggsave("display.3.age.png")
# AGE is already on the numerical scale !!  
student$age = as.integer(student$age)

################################################################################################
################################################################################################
# 4 address - student's home address type (binary: "U" - urban or "R" - rural)

# unique(student$address) ## [1] "U" "R"

# ggplot(data = student) + 
#        geom_bar(mapping = aes(x=address, fill=address))

# ggsave("display.4.address.png")
student$address = as.factor(student$address)

################################################################################################
################################################################################################
# 5 famsize - family size (binary: "LE3" - less or equal to 3 or "GT3" - greater than 3)

# unique(student$famsize)

# ggplot(data = student) + 
#        geom_bar(mapping = aes(x=famsize, fill=famsize))

# ggsave("display.5.famsize.png")
student$famsize = as.factor(student$famsize)

################################################################################################
################################################################################################
# 6 Pstatus - parent's cohabitation status (binary: "T" - living together or "A" - apart)

# unique(student$Pstatus)

# ggplot(data = student) + 
#        geom_bar(mapping = aes(x=Pstatus, fill=Pstatus))

# ggsave("display.6.Pstatus.png")
student$Pstatus = as.factor(student$Pstatus)

################################################################################################
################################################################################################
# 7 Medu - mother's education (numeric: 0 - none,  1 - primary education (4th grade), 
# 2 - 5th to 9th grade, 3 - secondary education or 4 - higher education)

# unique(student$Medu)

# ggplot(data = student) + 
#        geom_bar(mapping = aes(x=Medu, fill=Medu))

# ggsave("display.7.Medu.png")
# we may wanna use the numerical values in various regression models
student$Medu = as.integer(student$Medu)

################################################################################################
################################################################################################
# 8 Fedu - father's education (numeric: 0 - none,  1 - primary education (4th grade), 
# 2 - 5th to 9th grade, 3 - secondary education or 4 - higher education)

# unique(student$Fedu)

# ggplot(data = student) + 
#        geom_bar(mapping = aes(x=Fedu, fill=Fedu))

# ggsave("display.8.Fedu.png")
# we may wanna use the numerical values in various regression models
student$Fedu = as.integer(student$Fedu)

################################################################################################
################################################################################################
# 9 Mjob - mother's job (nominal: "teacher", "health" care related, civil "services" 
# (e.g. administrative or police), "at_home" or "other")

# unique(student$Mjob)

# ggplot(data = student) + 
#        geom_bar(mapping = aes(x=Mjob, fill=Mjob))

# ggsave("display.9.Mjob.png")
student$Mjob = as.factor(student$Mjob)

################################################################################################
################################################################################################
# 10 Fjob - father's job (nominal: "teacher", "health" care related, civil "services" 
# (e.g. administrative or police), "at_home" or "other")

# unique(student$Fjob)

# ggplot(data = student) + 
#        geom_bar(mapping = aes(x=Fjob, fill=Fjob))

# ggsave("display.10.Fjob.png")
student$Fjob = as.factor(student$Fjob)

################################################################################################
################################################################################################
# 11 reason - reason to choose this school 
# (nominal: close to "home", school "reputation", "course" preference or "other")

# unique(student$reason)

# ggplot(data = student) + 
#        geom_bar(mapping = aes(x=reason, fill=reason))

# ggsave("display.11.reason.png")
student$reason = as.factor(student$reason)

################################################################################################
################################################################################################
# 12 guardian - student's guardian (nominal: "mother", "father" or "other")

# unique(student$guardian)

# ggplot(data = student) + 
#        geom_bar(mapping = aes(x=guardian, fill=guardian))

# ggsave("display.12.guardian.png")
student$guardian = as.factor(student$guardian)

################################################################################################
################################################################################################
# 13 traveltime - home to school travel time (numeric: 1 - <15 min., 2 - 15 to 30 min., 
# 3 - 30 min. to 1 hour, or 4 - >1 hour)

# unique(student$traveltime)

# ggplot(data = student) + 
#       geom_bar(mapping = aes(x=traveltime, fill=traveltime))

# ggsave("display.13.traveltime.png")
# we may wanna use the NUMERICAL VALUES :
student$traveltime = as.integer(student$traveltime)

################################################################################################
################################################################################################
# 14 studytime - weekly study time (numeric: 1 - <2 hours, 2 - 2 to 5 hours, 
# 3 - 5 to 10 hours, or 4 - >10 hours)

# unique(student$studytime)

# ggplot(data = student) + 
#        geom_bar(mapping = aes(x=studytime, fill=studytime))

# ggsave("display.14.studytime.png")
# we may wanna use the NUMERICAL VALUES :
student$studytime = as.integer(student$studytime)

################################################################################################
################################################################################################
# 15 failures - number of past class failures (numeric: n if 1<=n<3, else 4)

# unique(student$failures)

# ggplot(data = student) + 
#       geom_bar(mapping = aes(x=failures, fill=failures))

# ggsave("display.15.failures.png")
# we may wanna use the NUMERICAL VALUES :
student$failures = as.integer(student$failures)

################################################################################################
################################################################################################
# 16 schoolsup - extra educational support (binary: yes or no)

# unique(student$schoolsup)

# ggplot(data = student) + 
#        geom_bar(mapping = aes(x=schoolsup, fill=schoolsup))

# ggsave("display.16.schoolsup.png")
student$schoolsup = as.factor(student$schoolsup)

################################################################################################
################################################################################################
# 17 famsup - family educational support (binary: yes or no)

# unique(student$famsup)

# ggplot(data = student) + 
#        geom_bar(mapping = aes(x=famsup, fill=famsup))

# ggsave("display.17.famsup.png")
student$famsup = as.factor(student$famsup)

################################################################################################
################################################################################################
# 18 paid - extra paid classes within the course subject (Math or Portuguese) 
# (binary: yes or no)

# unique(student$paid)

# ggplot(data = student) + 
#        geom_bar(mapping = aes(x=paid, fill=paid))

# ggsave("display.18.paid.png")
student$paid = as.factor(student$paid)

################################################################################################
################################################################################################
# 19 activities - extra-curricular activities (binary: yes or no)

# unique(student$activities)

# ggplot(data = student) + 
#        geom_bar(mapping = aes(x=activities, fill=activities))

# ggsave("display.19.activities.png")
student$activities = as.factor(student$activities)

################################################################################################
################################################################################################
# 20 nursery - attended nursery school (binary: yes or no)

# unique(student$nursery)

# ggplot(data = student) + 
#        geom_bar(mapping = aes(x=nursery, fill=nursery))

# ggsave("display.20.nursery.png")
student$nursery = as.factor(student$nursery)

################################################################################################
################################################################################################
# 21 higher - wants to take higher education (binary: yes or no)

# unique(student$higher)

# ggplot(data = student) + 
#        geom_bar(mapping = aes(x=higher, fill=higher))

# ggsave("display.21.higher.png")
student$higher = as.factor(student$higher)

################################################################################################
################################################################################################
# 22 internet - Internet access at home (binary: yes or no)

# unique(student$internet)

# ggplot(data = student) + 
#        geom_bar(mapping = aes(x=internet, fill=internet))

# ggsave("display.22.internet.png")
student$internet = as.factor(student$internet)

################################################################################################
################################################################################################
# 23 romantic - with a romantic relationship (binary: yes or no)

# unique(student$romantic)

# ggplot(data = student) + 
#        geom_bar(mapping = aes(x=romantic, fill=romantic))

# ggsave("display.23.romantic.png")
student$romantic = as.factor(student$romantic)

################################################################################################
################################################################################################
# 24 famrel - quality of family relationships (numeric: from 1 - very bad to 5 - excellent)

# unique(student$famrel)

# ggplot(data = student) + 
#        geom_bar(mapping = aes(x=famrel, fill=famrel))

# ggsave("display.24.famrel.png")
# i believe that we can keep these as numerical :
student$famrel = as.integer(student$famrel)

################################################################################################
################################################################################################
# 25 freetime - free time after school (numeric: from 1 - very low to 5 - very high)

# unique(student$freetime)

# ggplot(data = student) + 
#       geom_bar(mapping = aes(x=freetime, fill=freetime))

# ggsave("display.25.freetime.png")
# i believe that we can keep these as numerical :
student$freetime = as.integer(student$freetime)

################################################################################################
################################################################################################
# 26 goout - going out with friends (numeric: from 1 - very low to 5 - very high)

# unique(student$goout)

# ggplot(data = student) + 
#        geom_bar(mapping = aes(x=goout, fill=goout))

# ggsave("display.26.goout.png")
# i believe that we can keep these as numerical :
student$goout = as.integer(student$goout)

################################################################################################
################################################################################################
# 27 Dalc - workday alcohol consumption (numeric: from 1 - very low to 5 - very high)

# unique(student$Dalc)

# ggplot(data = student) + 
#        geom_bar(mapping = aes(x=Dalc, fill=Dalc))

# ggsave("display.27.Dalc.png")
# i believe that we can keep these as numerical :
student$Dalc = as.integer(student$Dalc)

################################################################################################
################################################################################################
# 28 Walc - weekend alcohol consumption (numeric: from 1 - very low to 5 - very high)

# unique(student$Walc)

# ggplot(data = student) + 
#        geom_bar(mapping = aes(x=Walc, fill=Walc))

# ggsave("display.28.Walc.png")
# i believe that we can keep these as numerical :
student$Walc = as.integer(student$Walc)

################################################################################################
################################################################################################
# 29 health - current health status (numeric: from 1 - very bad to 5 - very good)

# unique(student$health)

# ggplot(data = student) + 
#        geom_bar(mapping = aes(x=health, fill=health))

# ggsave("display.29.health.png")
# i believe that we can keep these as numerical :
student$health = as.integer(student$health)

################################################################################################
################################################################################################
# 30 absences - number of school absences (numeric: from 0 to 93)

# unique(student$absences)

# ggplot(data = student) + 
#       geom_bar(mapping = aes(x=absences, fill=absences))

# ggplot(data=student, aes(x=absences)) + 
#      geom_histogram(aes(y=..density..), colour="black", fill="white")+
#      geom_density(alpha=.2, fill="#FF6666")

# ggsave("display.30.absences.png")
# i believe that we can keep these as numerical :
student$absences = as.integer(student$absences)

################################################################################################
################################################################################################
# $ G1        : int  5 5 7 15 6 15 12 6 16 14 ...

# unique(student$G1)

# ggplot(data = student) + 
#        geom_bar(mapping = aes(x=G1, fill=G1))

# ggplot(data=student, aes(x=G1)) + 
#       geom_histogram(aes(y=..density..), colour="black", fill="white")+
#       geom_density(alpha=.2, fill="#FF6666")

# ggsave("display.0.G1.png")
# i believe that we can keep these as numerical, although we may not need it :
student$G1 = as.factor(student$G1)

################################################################################################
################################################################################################
# $ G2        : int  6 5 8 14 10 15 12 5 18 15 ...

# unique(student$G2)

# ggplot(data = student) + 
#        geom_bar(mapping = aes(x=G2, fill=G2))

# ggplot(data=student, aes(x=G2)) + 
#       geom_histogram(aes(y=..density..), colour="black", fill="white")+
#       geom_density(alpha=.2, fill="#FF6666")

# ggsave("display.0.G2.png")
# i believe that we can keep these as numerical, although we may not need it :
student$G2 = as.factor(student$G2)

################################################################################################
################################################################################################
# $ G3        : int  6 6 10 15 10 15 11 6 19 15 ...

# unique(student$G3)

# ggplot(data = student) + 
#        geom_bar(mapping = aes(x=G3, fill=G3))

# ggplot(data=student, aes(x=G3)) + 
#       geom_histogram(aes(y=..density..), colour="black", fill="white")+
#       geom_density(alpha=.2, fill="#FF6666")

# ggsave("display.0.G3.png")
# i believe that we can covert it into RANGES of VALUES :
student$G3 = as.factor(student$G3)

################################################################################################
summary(student)
str(student)
class(student)
################################################################################################
# knitr::kable(summary(student, format = "html"))
################################################################################################
```

\newpage
<p>&nbsp;</p>
#### **3. DATA SELECTION**
<p>&nbsp;</p>
\newpage

```{r }
## the OUTPUT VARIABLES is G3
## we may remove G1 and G2
## and some other features

student1 <- subset(student, select = -c(G1, G2))

student2 <- subset(student1, 
                   select = -c(school, sex, address, famsize, Pstatus, 
                   Mjob, Fjob, reason, guardian, schoolsup, famsup, 
                   paid, activities, nursery, 
                   higher, internet, romantic))

### shall we decide to keep ALL the FEATURES (ATTRIBUTES)
student2 = student1

str(student2)

student2$G3 = as.factor(student2$G3)

table(student2$G3)

### for simplicity, to work with a copy of STUDENT2, let's call it STUDENT3

student3 = subset(student2, 
                  select= c(age, traveltime, studytime, failures, absences, G3))

### shall we decide to keep ALL the FEATURES (ATTRIBUTES)
### student3 = student2

table(student3$G3)
```

\newpage
<p>&nbsp;</p>
#### **4. DATA FILTERING**
<p>&nbsp;</p>
\newpage

```{r } 
## in order to KEEP the RECORDS where the GRADE 3 is > 2 :

dim(student3)

student3$G3 = as.integer(student3$G3)

student4 = student3[student3$G3 > 2, ]

dim(student4) 

ggplot(data = student4) + 
       geom_bar(mapping = aes(x=G3, fill=G3))

ggsave("display.0.G3.after.filtering.grade3.frequency.png")

student3 = student4

## TRANSFORMING G3 into RANGES of PASS and NO-PASS :

student3$G3 = as.integer(student3$G3)

student3$RESULT[student3$G3 <= 10] = "NO_PASS"
student3$RESULT[student3$G3 >=10 ] = "PASS"

student3 <- subset(student3, select = -c(G3))

student3$RESULT = as.factor(student3$RESULT)

## DISPLAYING THE FEATURES (ATTRIBUTES) in THE CURRENT DATASET :

colnames(student3)
```

\newpage
<p>&nbsp;</p>
#### **5. TRAINING AND TEST SETS**
<p>&nbsp;</p>
\newpage

```{r } 
### CHOOSING the **TRAINING** and the **TESTING** SETS

set.seed(123)

indxTrain <- createDataPartition(student3$RESULT, 
                                 p = 0.70, 
                                 list = FALSE)

training <- student3[indxTrain,]
# training

testing <- student3[-indxTrain,]
# testing

dim(student3)
dim(training)
dim(testing)
```

\newpage
<p>&nbsp;</p>
#### **6. TRAINING AND PREDICTIONS WITH ANN (CARET)**
<p>&nbsp;</p>
\newpage

<p>&nbsp;</p>
#### **6.1. TRAINING**
<p>&nbsp;</p>

```{r } 
set.seed(123)

TrainingParameters <- trainControl(method = "repeatedcv", number = 10, repeats=10)

# nnnet package by defualt uses the Logistic Activation function
fit.nn <- train( RESULT~ ., 
                   data = training, 
                   method = "nnet", 
                   trControl = TrainingParameters, 
                   preProcess = c("center","scale"), 
                   trace=FALSE,
                   verbose=FALSE, 
                   # tuneLength = 20, 
                   na.action = na.omit)

## The OUTPUT of nnet

# Size: Number of Hidden Layers.
# Decay: Is the regularization factor that offsets overfitting.
# Kappa: Evaluates the match is significant or by chance.

head(fit.nn$results)
tail(fit.nn$results)

print(fit.nn)
# plot(fit.nn)
```

<p>&nbsp;</p>
#### **6.2. PREDICTIONS**
<p>&nbsp;</p>

```{r } 
## colnames(testing)
## [1] "age"        "traveltime" "studytime"  "failures"   "absences"  
## [6] "RESULT"
## nn_predict <- predict(nn_model, testing[-6])

fit.nn.predict <- predict(fit.nn, newdata = testing)
```

We would aim to optimize the model by FEATURE SELECTION or by including NEW FEATURES 
from the data that is available (we have excluded at the beginning many features). 

<p>&nbsp;</p>
#### **6.3. THE CONFUSION MATRIX**
<p>&nbsp;</p>

```{r } 
confusionMatrix(fit.nn.predict, testing$RESULT)
```

**The ACCURACY of the MODEL is :** 

```{r } 
mean(fit.nn.predict == testing$RESULT)

# dim(student3)
# accuracy <- sum(nn_predict == (testing$RESULT))/length(testing$RESULT)
# print(accuracy)
```

<p>&nbsp;</p>
#### **6.4. THE VARIABLE IMPORTANCE**
<p>&nbsp;</p>

```{r } 
X <- varImp(fit.nn)
print(X)
plot(X)

plotnet(fit.nn)
title("Graphical Representation of Neural Network")
```

\newpage
<p>&nbsp;</p>
#### **7. TRAINING AND PREDICTIONS WITH ANN**
<p>&nbsp;</p>
\newpage

We are using the package "neuralnet" available on CRAN : 

https://cran.r-project.org/web/packages/neuralnet/index.html

and according to the description in the book : 

"Machine Learning with R"

<p>&nbsp;</p>
#### **7.1. TRAINING**
<p>&nbsp;</p>

```{r } 
set.seed(123)

model.nn1 <- neuralnet(RESULT ~ age + traveltime + studytime + failures + absences,
                       data = training, 
                       hidden=2, 
                       act.fct = "logistic", 
                       linear.output = FALSE)

plot(model.nn1)

model.nn2 <- neuralnet(RESULT ~ age + traveltime + studytime + failures + absences,
                       data = training, 
                       hidden=2, 
                       act.fct = "tanh", 
                       linear.output = FALSE)

plot(model.nn2)
```

<p>&nbsp;</p>
#### **7.2. PREDICTIONS**
<p>&nbsp;</p>

```{r } 
set.seed(123)

model.nn1.results <- neuralnet::compute(model.nn1, testing)
head(model.nn1.results$net.result)

model.nn2.results <- neuralnet::compute(model.nn2, testing)
head(model.nn2.results$net.result)
```


We have followed also the materials from online resources : 

https://datascienceplus.com/neuralnet-train-and-test-neural-networks-using-r/

although using CARET package is simpler and easier, and permits also an easy 
calculation of the CONFUSION MATRIX and the VARIABLE IMPORTANCE.

\newpage
<p>&nbsp;</p>
#### **8. TRAINING AND PREDICTIONS WITH SVM (CARET)**
<p>&nbsp;</p>
\newpage

\newpage
<p>&nbsp;</p>
#### **8.A. using SVM_LINEAR**
<p>&nbsp;</p>
\newpage

<p>&nbsp;</p>
#### **8.1. TRAINING**
<p>&nbsp;</p>

```{r } 
set.seed(123)

TrainingParameters <- trainControl(method = "repeatedcv", number = 10, repeats=10)
# grid <- expand.grid(C = c(0,0.01, 0.05, 0.1, 0.25, 0.5, 0.75, 1, 1.25, 1.5, 1.75, 2,5))
                    
svm_Linear <- train( RESULT~ ., 
                     data = training, 
                     method = "svmLinear", 
                     trControl = TrainingParameters, 
                     preProcess = c("center","scale"), 
                     trace=FALSE,
                     verbose=FALSE, 
                     # tuneGrid = grid,
                     # tuneLength = 20, 
                     na.action = na.omit)

## The OUTPUT of svm_Linear

head(svm_Linear$results)
tail(svm_Linear$results)

print(svm_Linear)
# plot(svm_Linear)
```

<p>&nbsp;</p>
#### **8.2. PREDICTIONS**
<p>&nbsp;</p>

```{r } 
set.seed(123)
svm_Linear_predict <- predict(svm_Linear, newdata = testing)
```

<p>&nbsp;</p>
#### **8.3. THE CONFUSION MATRIX**
<p>&nbsp;</p>

```{r } 
set.seed(123)
confusionMatrix(svm_Linear_predict, testing$RESULT)
```

**The ACCURACY of the MODEL is :** 

```{r } 
mean(svm_Linear_predict == testing$RESULT)
```

<p>&nbsp;</p>
#### **8.4. THE VARIABLE IMPORTANCE**
<p>&nbsp;</p>

```{r } 
X <- varImp(svm_Linear)
print(X)
plot(X)
```

\newpage
<p>&nbsp;</p>
#### **8.B. using SVM_RADIAL**
<p>&nbsp;</p>
\newpage

<p>&nbsp;</p>
#### **8.5. TRAINING**
<p>&nbsp;</p>

```{r } 
set.seed(123)

TrainingParameters <- trainControl(method = "repeatedcv", number = 10, repeats=10)
# grid <- expand.grid(C = c(0,0.01, 0.05, 0.1, 0.25, 0.5, 0.75, 1, 1.25, 1.5, 1.75, 2, 5))

svm_Radial <- train( RESULT~ ., 
                   data = training, 
                   method = "svmRadial", 
                   trControl = TrainingParameters, 
                   preProcess = c("center","scale"), 
                   trace=FALSE,
                   verbose=FALSE, 
                   # tuneGrid = grid,
                   # tuneLength = 20, 
                   na.action = na.omit)

## The OUTPUT of svm_Radial

head(svm_Radial$results)
tail(svm_Radial$results)

print(svm_Radial)
plot(svm_Radial)
```

<p>&nbsp;</p>
#### **8.6. PREDICTIONS**
<p>&nbsp;</p>

```{r } 
set.seed(123)
svm_Radial_predict <- predict(svm_Radial, newdata = testing)
```

<p>&nbsp;</p>
#### **8.7. THE CONFUSION MATRIX**
<p>&nbsp;</p>

```{r } 
set.seed(123)
confusionMatrix(svm_Radial_predict, testing$RESULT)
```

**The ACCURACY of the MODEL is :** 

```{r } 
mean(svm_Radial_predict == testing$RESULT)
```

<p>&nbsp;</p>
#### **8.8. THE VARIABLE IMPORTANCE**
<p>&nbsp;</p>

```{r } 
X <- varImp(svm_Radial)
print(X)
plot(X)
```

\newpage
<p>&nbsp;</p>
#### **9. TRAINING AND PREDICTIONS WITH SVM**
<p>&nbsp;</p>
\newpage


As it is described in the book of the class "Machine Learning with R", we may also use the package "kernlab" where ksvm() function uses the Gaussian RBF kernel (by default), 

or it may use the following other kernels:

• ‘rbfdot’ Radial Basis kernel "Gaussian"

• ‘polydot’ Polynomial kernel

• ‘vanilladot’ Linear kernel

• ‘tanhdot’ Hyperbolic tangent kernel

• ‘laplacedot’ Laplacian kernel

• ‘besseldot’ Bessel kernel

• ‘anovadot’ ANOVA RBF kernel

• ‘splinedot’ Spline kernel

• ‘stringdot’ String kernel

We have chosen to work below with **rbfdot** and **tanhdot**. 

<p>&nbsp;</p>
#### **9.1. TRAINING**
<p>&nbsp;</p>

```{r } 
suppressPackageStartupMessages(library(klaR))
suppressPackageStartupMessages(library(kernlab))

set.seed(123)

model.ksvm1 <- ksvm(RESULT ~ age + traveltime + studytime + failures + absences, 
                    data = training, 
                    kernel="rbfdot")

model.ksvm1

model.ksvm2 <- ksvm(RESULT ~ age + traveltime + studytime + failures + absences, 
                    data = training, 
                    kernel="tanhdot")

model.ksvm2

```

<p>&nbsp;</p>
#### **9.2. PREDICTIONS**
<p>&nbsp;</p>

```{r } 
set.seed(123)

model.ksvm1.results <- predict(model.ksvm1, testing, type="response")
head(model.ksvm1.results)

table(model.ksvm1.results, testing$RESULT)
agreement1 <-  model.ksvm1.results == testing$RESULT
table(agreement1)
prop.table(table(agreement1))

model.ksvm2.results <- predict(model.ksvm2, testing, type="response")
head(model.ksvm2.results)

table(model.ksvm2.results, testing$RESULT)
agreement2 <-  model.ksvm2.results == testing$RESULT
table(agreement2)
prop.table(table(agreement2))
```

We have followed above the materials from the book recommended in the class : 

https://www.amazon.com/Machine-Learning-R-Brett-Lantz-ebook/dp/B00G9581JM

although using CARET package is simpler and easier, and permits also a direct
calculation of the CONFUSION MATRIX and VARIABLE IMPORTANCE.

\newpage
<p>&nbsp;</p>
#### **10. CONCLUSIONS**
<p>&nbsp;</p>
\newpage

Here below we are comparing the algorithms that we have used above, 

particularly ANN, 

and SVM with a Linear Kernel, 

and SVM with a Radial Basis Function (RBF) Kernel.

```{r } 
set.seed(123)

algo_results <- resamples(list(NNET=fit.nn, 
                               SVML=svm_Linear, 
                               SVMR=svm_Radial))

summary(algo_results)

scales <- list(x=list(relation="free"), y=list(relation="free"))

bwplot(algo_results, scales=scales)

splom(algo_results)

diffs <- diff(algo_results)

# summarize p-values for pair-wise comparisons

summary(diffs)
```


As we can see, by comparing the **ACCURACY** on Box-and-Whisker plots, the ML model that is
based on SVM-RBF performs better than the ML models that are based on ANN and SVM-LK, 
although the precise **ACCURACY** that we have obtained with SVM-RBF is only 0.58.


In fact, considering the precise values of the **ACCURACY**, we could rank **ANN** (0.62),
followed by **SVM-LK** (0.61), and **SVM-RBF** (0.58).    


Also the **FEATURES** that are considered as important differ between these three models :


in **ANN** model, the order of feature importance is : 

"studytime", failures", "absences", "age" ;


in sharp contrast with the model based on **SVM (LK or RBF)**, that place more emphasis on :

"absences", "age", "failures", and "traveltime" (SVM-LK and SVM-RBF). 

\newpage

<p>&nbsp;</p>
#### **10. adding another models**
<p>&nbsp;</p>

\newpage

<p>&nbsp;</p>
#### **4. DATA SUMMARY and VISUALIZATION**
<p>&nbsp;</p>

In the section 4, we aim to address the following Q1 from the course.

**STEP 1 Data Descriptive Statistics**

**Q1. Amongst the variables of interest identify one that is categorical and one that is quantitative and then provide the following descriptive deliverables:**

**Summaries (Do this for at least one categorical and one quantitative variable).**

a) For the categorical variable create a frequency distribution.

b) For the categorical variable create a bar diagram.

c) For the quantitative variable create numerical summaries grouped by a categorical variable.

d) For the quantitative variable create a histogram and a boxplot grouped by categorical variable.

\newpage

<p>&nbsp;</p>
#### **4. DATA SUMMARY and VISUALIZATION**
<p>&nbsp;</p>

**We display the GRADE G3 (NO PASS/PASS), function of AGE**

```{r } 
## after we REMOVE the RECORDS where the GRADE G3 is > 2 ;
## we add a new piece of R code where we display the GRADE G3, function of AGE

student3 %>%
  group_by(RESULT, age) %>%
  summarise (n = n()) %>%
  mutate(freq = n / sum(n)) 
# %>% arrange(desc(freq))

student3 %>%
  group_by(RESULT, age) %>%
  tally() %>%
  arrange(desc(n)) 

student3 %>%
  group_by(RESULT, age) %>%
  tally() %>%
  arrange(desc(n)) %>%
ggplot(aes(x = age, y=n)) +
       geom_bar(stat="identity", aes(fill=age)) + 
       facet_wrap(~RESULT)

student3 %>%
  group_by(RESULT, age) %>%
  tally() %>%
  # arrange(desc(n)) %>%
ggplot(aes(x = RESULT, y=n)) +
       geom_boxplot(aes(fill=RESULT)) 

```

\newpage

<p>&nbsp;</p>
#### **4. DATA SUMMARY and VISUALIZATION**
<p>&nbsp;</p>
**We display the GRADE G3 (NO PASS/PASS), function of ABSENCES**

```{r } 
## after we REMOVE the RECORDS where the GRADE G3 is > 2 ;
## we add a new piece of R code where we display the GRADE G3, function of ABSENCES

student3 %>%
  group_by(RESULT, absences) %>%
  summarise (n = n()) %>%
  mutate(freq = n / sum(n)) 
# %>% arrange(desc(freq))

student3 %>%
  group_by(RESULT, absences) %>%
  tally() %>%
  arrange(desc(n)) 

student3 %>%
  group_by(RESULT, absences) %>%
  tally() %>%
  arrange(desc(n)) %>%
ggplot(aes(x = absences, y=n)) +
       geom_bar(stat="identity", aes(fill=absences)) + 
      facet_wrap(~RESULT)

student3 %>%
  group_by(RESULT, absences) %>%
  tally() %>%
  # arrange(desc(n)) %>%
ggplot(aes(x = RESULT, y=n)) +
       geom_boxplot(aes(fill=RESULT)) 

```

\newpage

<p>&nbsp;</p>
#### **4. DATA SUMMARY and VISUALIZATION : the CORRELATION PLOTS**
<p>&nbsp;</p>

We aim to address the following Q2 from the course 

**STEP 2 Correlation and Regression Analysis**

although the data that we have chosen and the numerical features does not allow us a classical regression analysis. We will do it, just to set up the code in R (for other datasets).

**Q2. Among the quantitative variables generate Relationships and Associations.** 

Correlation and Regression:

a) Identify two or more quantitative variables that might be correlated.

b) Find the correlation coefficient.

c) Create the scatter diagram under graphs.

d) Provide your rationale and justify your findings regarding the correlation between two quantitative variables of interest.

Here, we aim to answer also the question Q3 : 

**Q3.Prepare data by using the following preprocessing transformation and plots:**

a) Please standardize the data. 

b) Check for null values

c) Check for outliers

d) Check for Regression assumptions generate regression diagnostic plots.

\newpage

<p>&nbsp;</p>
#### **4. DATA SUMMARY and VISUALIZATION : the CORRELATION PLOTS**
<p>&nbsp;</p>

**We display the SCATTER PLOTS between the numerical features that we have the dataset i.e. AGE and ABSENCES (although the SCATTER PLOTS looks atypical for the data that we have chosen).**

```{r } 
# library(Hmisc)
suppressMessages(library(Hmisc))

# computing the CORRELATION COEFFICIENT between AGE and ABSENCES ;
# we find a SMALL CORRELATION COEFFICIENT (< 0.3)

cor(student3$age, student3$absences)

```

As the CORRELATION COEFFICIENT is small (<0.3), we can keep both AGE and ABSENCES in the model, 
as **INDEPENDENT FEATURES** (we know that some ML approaches are sensitive to features that are
highly correlated).

```{r } 
cov(student3$age, student3$absences)
```

```{r } 
# a SCATTER PLOT using geom_smooth()

# ggplot(student3, aes(x=age, y=absences)) +
#       geom_point(size=2) + 
#       geom_smooth()

# a SCATTER PLOT using geom_smooth(method=lm)

ggplot(student3, aes(x=age, y=absences)) +
       geom_point(size=2) + 
       geom_smooth(method=lm)
```

As an exercise in this section, as we look at the correlation between **AGE** and **ABSENCES**, we also perform a more formal linear regression analysis and compute the **DIAGNOSTIC PLOTS**.

```{r } 
library(broom) ### in order to add : AUGMENT

## A LM approach :

reg_model <- lm(absences~age, data = student3)

reg_model

## Listing R.squared in the LM approach :

summary(reg_model)$r.squared 

## Making the Diagnostic Plots:

reg_model.diagnostics <- augment(reg_model)

head(reg_model.diagnostics)

## Another view at the data ;
## potentially to identify the outlier values :

ggplot(reg_model.diagnostics, aes(age, absences)) +
geom_point() +
stat_smooth(method = lm, se = FALSE) +
geom_segment(aes(xend = age, yend = .fitted), color = "red", size = 0.3)

## A view at the LINEAR REGRESSION RESULTS :

par(mfrow = c(2, 2))
plot(reg_model)

## Another view at the LINEAR REGRESSION RESULTS :

library(ggfortify) 
autoplot(reg_model)
```

As we can see in the **Q-Q PLOT**, the distribution of the data is not GAUSSIAN, 
and there are 3 OUTLIER POINTS that have the INDEXES 75, 184, and 277 (below).


```{r } 
## Indeed, the DATA POINTS on ABSENCES that have the INDEXES 75, 184, 277, 
## are the TOP OULIERS, and we may wanna remove these OUTLIERS from the data. 

student3[75,]
# student3[184,]
# student3[277,]
```

We also use several methods for normality testing such as **Kolmogorov-Smirnov (K-S) normality test** and **Shapiro-Wilk’s test**. As we see below, the hypothesis of "normality" is rejected for both features "age" and "absences". 

```{r } 
library(ggpubr)
ggqqplot(student3$age)

shapiro.test(student3$age)
ks.test(student3$age, "pnorm")

library(ggpubr)
ggqqplot(student3$absences)

shapiro.test(student3$absences)
ks.test(student3$absences, "pnorm")
```

\newpage

<p>&nbsp;</p>
#### **5. TRAINING AND TEST SETS**
<p>&nbsp;</p>

```{r } 
## CHOOSING the TRAINING and the TESTING SETS

indxTrain <- createDataPartition(student3$RESULT, 
                                 p = .70, 
                                 list = FALSE)

training <- student3[indxTrain,]
# training

testing <- student3[-indxTrain,]
# testing

dim(student3)
dim(training)
dim(testing)
```

\newpage

<p>&nbsp;</p>
#### **6. PRE-PROCESSING THE DATA**
<p>&nbsp;</p>

We can pre-process the data in a manner that is shown below, by using the COMMAND 
"preProcess" and "method = c("center", "scale")", although it is likely easier to do 
the pre-processing by using the option "preProcess = c("center","scale")" in *train()*.

```{r } 
## PRE-PROCESSING the DATA

trainX        <- training[, names(training) != "RESULT"]

preProcValues <- preProcess(x = trainX, method = c("center", "scale"))

# preProcValues

names(trainX)
dim(trainX)

names(training)
names(testing)

scaledTrain <- predict(preProcValues, trainX)
```

\newpage

<p>&nbsp;</p>
#### **7. PERFORMING THE TRAINING**
<p>&nbsp;</p>

We cover in the following sections the following :   

**Step 4 Implement Regression and Decision Trees**

Conduct Regression and answer the following questions:

**Q4 Implement Regression and Decision Tree.**

a) Objective and rationale of using the specific algorithm to achieve the objective.

b) Steps of implementing the algorithm with regards to the context.

c) Interpretation of the results and prediction accuracy achieved.

d) Performance improvement techniques and improved accuracy achieved. 

Use feature selection, variable importance, compare RMSE(Regression) across models and 
Information gain (Decision Trees), K-fold cross validation, grid search etc.

e) Implement the two algorithms and state the insights obtained from the implemented project.

\newpage 

<p>&nbsp;</p>
#### **7. PERFORMING THE TRAINING**
<p>&nbsp;</p>

```{r } 
## PERFORMING the TRAINING

set.seed(400)
ctrl <- trainControl(method="repeatedcv", repeats = 3) 

rpartFit <- train( RESULT~ ., 
                 data = training, 
                 method = "rpart", 
                 trControl = ctrl, 
                 preProcess = c("center","scale"), tuneLength = 20)

## The output of rpartFit

rpartFit

## summary(rpartFit$finalModel)
## it outputs a very long summary

## The plot of rpartFit

plot(rpartFit)

png("the.results.rpartFIT.png")
plot(rpartFit)
dev.off()

## To look at the VARIABLE IMPORTANCE

X <- varImp(rpartFit)
plot(X)
```

As we can note, in the current model, **more important FEATURES are AGE, ABSENCES, and STUDY TIME.**

```{r } 
### DISPLAYING THE TREE

plot(rpartFit$finalModel, 
    uniform=TRUE,
    main="Classification Tree")
text(rpartFit$finalModel, use.n.=TRUE, all=TRUE, cex=.8)

png("the.results.rpartFIT.finalModel.png")
plot(rpartFit$finalModel, 
    uniform=TRUE,
    main="Classification Tree")
text(rpartFit$finalModel, use.n.=TRUE, all=TRUE, cex=.8)
dev.off()

# library(rattle)
suppressMessages(library(rattle))

fancyRpartPlot(rpartFit$finalModel)

png("the.results.rpartFIT.fancyR.png")
fancyRpartPlot(rpartFit$finalModel)
dev.off()

```

\newpage

<p>&nbsp;</p>
#### **8. MAKING THE PREDICTIONS**
<p>&nbsp;</p>

```{r } 
## Making the PREDICTIONS :

rpartPredict <- predict(rpartFit, newdata = testing)

# rpartPredict
```

We may aim to optimize the model by FEATURE SELECTION or by including NEW FEATURES 
from the data that is available (we have excluded at the beginning many fetures). 

\newpage

<p>&nbsp;</p>
#### **9. THE CONFUSION MATRIX**
<p>&nbsp;</p>

```{r } 
## COMPUTING the CONFUSION MATRIX :

confusionMatrix(rpartPredict, testing$RESULT)

mean(rpartPredict == testing$RESULT)

dim(student3)
```

**The ACCURACY of the MODEL based on DECISION TREES is :** 

```{r } 
mean(rpartPredict == testing$RESULT)
```

\newpage

<p>&nbsp;</p>
#### **10. CONCLUSIONS AND OTHER MODELS**
<p>&nbsp;</p>

Because the LABEL is BINARY (PASS/NO PASS), we can compare the performance of the model above with
the performance of a model that is based on **LOGISTIC REGRESSION**.

```{r } 

logisticFit = train( RESULT ~ .,
  data = training,
  trControl = ctrl,
  method = "glm",
  family = "binomial", 
  preProcess = c("center","scale"), tuneLength = 20)

## To look at the VARIABLE IMPORTANCE

X <- varImp(logisticFit)
plot(X)

## To compute the PREDICTIONS 
logisticPredict <- predict(logisticFit, newdata = testing)

## To display the CONFUSION MATRIX 
confusionMatrix(logisticPredict, testing$RESULT)

mean(logisticPredict == testing$RESULT)

```

**The ACCURACY of the MODEL based on LOGISTIC REGRESSION is :** 

```{r } 
mean(logisticPredict == testing$RESULT)
```

As we can see, by comparing the **ACCURACY**, the ML model that is based on **DECISION TREES** 
performs better than the ML model that is based on **LOGISTIC REGRESSION**.

Also the **FEATURES** that are considered as important differ between these two models : 
the LOGISTIC REGRESSION model emphasizes more on "failures", "absences", and "traveltime", 
and less on "studytime" and on "age", in sharp contrast with the model based on DECISION TREES.

A note to add about the model based on DECISION TREES, we do not have to standardize the data.. 

<p>&nbsp;</p>
\newpage


```{r warning=FALSE} 
## CHOOSING the TRAINING and TESTING SETS

indxTrain <- createDataPartition(student3$RESULT, 
                                 p = .75, 
                                 list = FALSE)

training <- student3[indxTrain,]
# head(training)

testing <- student3[-indxTrain,]
# head(testing)

dim(student3)
dim(training)
dim(testing)
```

<p>&nbsp;</p>
#### **6. PRE-PROCESSING THE DATA**
<p>&nbsp;</p>

```{r warning=FALSE} 
### PRE-PROCESSING the DATA

trainX        <- training[, names(training) != "RESULT"]

# for NB we may not need to CENTER and SCALE the data :)
# preProcValues <- preProcess(x = trainX, method = c("center", "scale"))
# preProcValues

names(trainX)

dim(trainX)

names(training)

### THE BALANCE of the DATA in TRAINING and TESTING SETS

prop.table(table(training$RESULT)) * 100

prop.table(table(testing$RESULT)) * 100

```

<p>&nbsp;</p>
#### **7. PERFORMING THE TRAINING**
<p>&nbsp;</p>

```{r warning=FALSE} 
### PERFORMING the TRAINING

set.seed(400)
ctrl <- trainControl(method="repeatedcv", repeats = 10) 

nbFit = train( RESULT~ ., 
                 data = training, 
                 method = "nb", 
                 trControl = ctrl) 

## The output of nbFit fit

nbFit

plot(nbFit)

png("the.results.nb.FIT.png")
plot(nbFit)
dev.off()
```

<p>&nbsp;</p>
#### **8. MAKING THE PREDICTIONS**
<p>&nbsp;</p>

```{r warning=FALSE} 
### Making the PREDICTIONS :

nbPredict <- predict(nbFit, newdata = testing)

```

<p>&nbsp;</p>
#### **9. THE CONFUSION MATRIX (caret package)**
<p>&nbsp;</p>

```{r warning=FALSE} 
### COMPUTING the CONFUSION MATRIX :

confusionMatrix(nbPredict, testing$RESULT)

mean(nbPredict == testing$RESULT)

dim(student3)

```

<p>&nbsp;</p>

<p>&nbsp;</p>

```{r warning=FALSE} 
# We implement the NB model also in other packages ("klaR", "e1071").
# here only another version of the R code

# library(e1071)

# x = training[, -6]
# y = training$RESULT

# model = train(x, y, 'nb', trControl=trainControl(method='cv',number=10))

# predict(model$finalModel,x)
# head(predict(model$finalModel,x)$class)

# table(predict(model$finalModel,x)$class,y)

# Predict <- predict(model, newdata = testing )

# We draw a plot that shows how each predictor variable is independently 
# responsible for predicting the outcome.
	
# to display Variable Performance
# X <- varImp(model)
# plot(X)

# the confusion matrix to see accuracy value and other parameter values
# confusionMatrix(Predict, testing$RESULT)

X <- varImp(nbFit)
plot(X)

```
<p>&nbsp;</p>

<p>&nbsp;</p>
#### **10. THE RESULTS (klaR package) **
<p>&nbsp;</p>

```{r warning=FALSE} 
### looking at the CORRELATIONS between the FEATURES 
library(GGally)

ggpairs(training)
ggpairs(testing)

```

```{r warning=FALSE} 
### using KLAR PACKAGE
library(klaR)

model = NaiveBayes( RESULT~ ., data = training)

predictions <- model %>% predict(testing)

# The ACCURACY
mean(predictions$class == testing$RESULT) 

```
<p>&nbsp;</p>

**As we can see, shall we set up the ML approach with NB, 
the accuracies of our models are almost equal and not too great.** 

<p>&nbsp;</p>
#### **5. TRAINING AND TEST SETS**
<p>&nbsp;</p>

```{r } 
## CHOOSING the TRAINING and TESTING SETS

indxTrain <- createDataPartition(student3$RESULT, 
                                 p = .75, 
                                 list = FALSE)

training <- student3[indxTrain,]
# training

testing <- student3[-indxTrain,]
# testing

dim(student3)
dim(training)
dim(testing)
```

<p>&nbsp;</p>
#### **6. PRE-PROCESSING THE DATA**
<p>&nbsp;</p>

```{r } 
## PRE-PROCESSING the DATA

trainX        <- training[, names(training) != "RESULT"]

preProcValues <- preProcess(x = trainX, method = c("center", "scale"))

preProcValues

names(trainX)

dim(trainX)

names(training)
```

<p>&nbsp;</p>
#### **7. PERFORMING THE TRAINING**
<p>&nbsp;</p>

```{r } 
## PERFORMING the TRAINING

set.seed(400)
ctrl <- trainControl(method="repeatedcv",repeats = 3) 

knnFit <- train( RESULT~ ., 
                 data = training, 
                 method = "knn", 
                 trControl = ctrl, 
                 preProcess = c("center","scale"), tuneLength = 20)

## The output of kNN fit

knnFit

png("the.results.knn.FIT.png")
plot(knnFit)
dev.off()

```

<p>&nbsp;</p>
#### **8. MAKING THE PREDICTIONS**
<p>&nbsp;</p>

```{r } 
## Making the PREDICTIONS :

knnPredict <- predict(knnFit, newdata = testing)

```
<p>&nbsp;</p>
#### **9. THE CONFUSION MATRIX**
<p>&nbsp;</p>

```{r } 
## COMPUTING the CONFUSION MATRIX :

confusionMatrix(knnPredict, testing$RESULT)

mean(knnPredict == testing$RESULT)

dim(student3)

```

<p>&nbsp;</p>

We may aim to optimize the model by feature selection or by including new features from the data that is available.

<p>&nbsp;</p>

IN THE NEXT SECTIONS to ADD the ENSEMBLE METHODS and the SUPER LEARNERS ..